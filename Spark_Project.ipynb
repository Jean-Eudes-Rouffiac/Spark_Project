{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Jean-Eudes Rouffiac  \n",
    "\n",
    "&nbsp;\n",
    "\n",
    "&nbsp;"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <center>  TP calcul distribué : classification de sentiments</center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "&nbsp;\n",
    "\n",
    "<p style=\"text-align:justify;\"> L'objectif de ce projet est d'entraîner un algorithme de classification de sentiments à l'aide d'un Framework distribué. Nous utiliserons pour ce projet la librairie PySpark. \n",
    "Nous utiliserons pour ce faire les fichiers suivants : </p>\n",
    "\n",
    "* train.json contenant le dataset d'entraînement\n",
    "* test.json contenant le dataset de test\n",
    "* noclass.json sur lequel il faudra effectuer les prédictions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "&nbsp;\n",
    "\n",
    "<p style=\"text-align:justify;\">Dans un premier temps, on importe les packages et fonctions qui vont être utiles pour réaliser la classification. </p>\n",
    "\n",
    "&nbsp;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import findspark\n",
    "\n",
    "\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql import SQLContext\n",
    "from pyspark import SparkContext\n",
    "from pyspark.ml.feature import HashingTF, IDF, Tokenizer, StringIndexer, StopWordsRemover\n",
    "from pyspark.ml import Pipeline\n",
    "from pyspark.ml.feature import StringIndexer\n",
    "from pyspark.sql import functions as fn\n",
    "\n",
    "\n",
    "\n",
    "from pyspark.ml import Pipeline\n",
    "from pyspark.ml.feature import StringIndexer, OneHotEncoderEstimator, VectorAssembler\n",
    "from pyspark.ml.feature import StopWordsRemover, Word2Vec, RegexTokenizer\n",
    "from pyspark.ml.classification import LogisticRegression\n",
    "from pyspark.sql import Row"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "&nbsp;\n",
    "\n",
    "<p style=\"text-align:justify;\">On instancie alors un objet SparkContext qui permet de gérer les propriétés globales de l'application, tel que le niveau de parallélisation par défaut. Le SparkContext permet également de charger les données en provenance de diverses sources de données.</p>\n",
    "\n",
    "&nbsp;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "sc =SparkContext()\n",
    "sqlContext = SQLContext(sc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "&nbsp;\n",
    "\n",
    "<p style=\"text-align:justify;\">Nous avons ici des fichiers .json, on les charge alors dans deux variables df_train et df_test.</p>\n",
    "\n",
    "&nbsp;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark = SparkSession.builder.appName('ops').getOrCreate()\n",
    "df_train = spark.read.json(\"train.json\")\n",
    "df_test = spark.read.json(\"test.json\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "&nbsp;\n",
    "\n",
    "<p style=\"text-align:justify;\">df_train et df_test sont deux dataframes. Ces dataframes de sqlContext sont l'essence même des distributions dans spark. Toutes les opérations (transformations etc) sur ces dataframes seront distribuées.\n",
    "On peut alors afficher les 5 premières lignes du dataset d'entraînement afin de voir à quoi ressemblent les données.</p>\n",
    "\n",
    "&nbsp;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+--------+\n",
      "|             message|polarity|\n",
      "+--------------------+--------+\n",
      "|! Comment était l...|       0|\n",
      "|! d'accord! Va-t-...|       0|\n",
      "|!!! Taihen desu n...|       0|\n",
      "|!!!! Auto-dj .. c...|       0|\n",
      "|!!!! Ce n'est que...|       0|\n",
      "+--------------------+--------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_train.show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "&nbsp;\n",
    "\n",
    "<p style=\"text-align:justify;\">On peut voir que les datasets sont composées de deux colonnes : une colonne contenant le message, et une colonne indiquant si le message est négatif (0) ou positif (4).</p>\n",
    "\n",
    "&nbsp;\n",
    "\n",
    "<p style=\"text-align:justify;\">Affichons maintenant le type des deux colonnes.</p>\n",
    "\n",
    "&nbsp;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- message: string (nullable = true)\n",
      " |-- polarity: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_train.printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "&nbsp;\n",
    "\n",
    "<p style=\"text-align:justify;\">On remarque que la colonne \"polarity\" est de type string alors qu'elle prend pour valeur soit 0 soit 5. On doit donc la convertir en entier, pour les deux datasets.</p>\n",
    "\n",
    "&nbsp;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- message: string (nullable = true)\n",
      " |-- polarity: integer (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_train = df_train.withColumn(\"polarity\", df_train[\"polarity\"].cast('int'))\n",
    "df_test = df_test.withColumn(\"polarity\", df_test[\"polarity\"].cast('int'))\n",
    "df_train.printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "&nbsp;\n",
    "\n",
    "<p style=\"text-align:justify;\">Affichons maintenant le nombre de messages positifs et le nombre de messages négatifs dans le dataset d'entraînement.</p>\n",
    "\n",
    "&nbsp;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+-----+\n",
      "|polarity|count|\n",
      "+--------+-----+\n",
      "|       4|66926|\n",
      "|       0|61475|\n",
      "+--------+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_train.groupBy(\"polarity\") \\\n",
    "    .count() \\\n",
    "    .orderBy(fn.col(\"count\").desc()) \\\n",
    "    .show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p style=\"text-align:justify;\">On remarque tout d'abord que le dataset est composé d'un très grand nombre de données et qu'en plus les deux classes sont présentes en grande quantité (+60k pour les deux).</p>\n",
    "\n",
    "&nbsp;\n",
    "\n",
    "## Pipeline\n",
    "\n",
    "&nbsp;\n",
    "<p style=\"text-align:justify;\">Nous allons maintenant passer à la partie classification. Nous utiliserons un outil très pratique qui est les pipeline. Cela permet de moduler facilement les différentes étapes du code. On défini plusieurs étapes en indiquant la colonne d'entrée et la colonne de sortie, puis on définit la pipeline en indiquant les différentes étapes. Il suffira alors de \"fit\" la pipeline puis de transformer les jeux de données pour obtenir les prédictions.</p>\n",
    "\n",
    "&nbsp;\n",
    "\n",
    "<p style=\"text-align:justify;\">Dans la partie de code suivante, nous importerons les fonctions nécessaires pour traiter le problème et nous définierons les étapes qui serviront à notre pipeline. Nous utiliserons les étapes suivantes (nous ferons plusieurs pipelines avec des algorithmes et des words embedding différents afin de sélectionner le word embedding et la méthode qui permet d'obtenir les meilleurs résultats) : </p>\n",
    "\n",
    "* Pour la partie \"nettoyage\" des données :\n",
    "    * regexTokenizer : permet de spliter le texte de la colonne 'message'.\n",
    "    * StopWordsRemover : permet de supprimer les stopwords. Malheureusement cela n'existe que pour les mots anglais... Nous ajoutons alors à la liste des stop words des mots ou caractères repérés dans la colonne message qui apporte du bruit.\n",
    "* Pour la partie word embedding : \n",
    "    * CountVectors : transforme la colonne obtenue après avoir enlever les stop words en une nouvelle colonne ou apparaîtra pour chaque message la taille du vocabulaire, ainsi que pour chaque mot du vocabulaire total (on prend tous les messages du dataset), le nombre de fois que le mot est employé. Cela permet de transformer le message en vecteur qui sera interprétable par un algorithme. Par exemple si le message contient plusieurs fois les mots \"ne\" et \"pas\", alors il aura plus de chance d'être interprêté comme un message négatif.\n",
    "    * HashingTF and IDF qui permettent de faire un TF-IDF.\n",
    "    * Word2Vec qui permet de convertir chaque mot en un vecteur de taille n (à définir). Ainsi il y a maintenant des notions de distance entre les mots.\n",
    "* Pour la partie modèle :\n",
    "    * logisticRegression\n",
    "    * random forest\n",
    "    * naive bayes\n",
    "    \n",
    "\n",
    "<p style=\"text-align:justify;\">Nous allons alors enchaîner les pipelines afin de garder celui qui a le meilleur résultat en classification sur les données test.</p>\n",
    "\n",
    "&nbsp;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from pyspark.ml.feature import RegexTokenizer, StopWordsRemover, CountVectorizer\n",
    "from pyspark.ml.classification import LogisticRegression, RandomForestClassifier, NaiveBayes\n",
    "from pyspark.ml import Pipeline\n",
    "from pyspark.ml.feature import OneHotEncoder, StringIndexer, VectorAssembler\n",
    "from pyspark.ml.evaluation import MulticlassClassificationEvaluator\n",
    "\n",
    "regexTokenizer = RegexTokenizer(inputCol=\"message\", outputCol=\"words\", pattern=\"\\\\W\")\n",
    "add_stopwords = [\"http\",\"https\",\"amp\",\"rt\",\"t\",\"c\",\"the\",\"null\",\"&\",\"Gt\",\"Lt\",\"gt\",\"lt\",\"Quot\", \"quot\",\"-\",\"*\",\".\",\"!\",\"?\"] \n",
    "stopwordsRemover = StopWordsRemover(inputCol=\"words\", outputCol=\"filtered\").setStopWords(add_stopwords)\n",
    "countVectors = CountVectorizer(inputCol=\"filtered\", outputCol=\"features\", vocabSize=10000, minDF=5)\n",
    "hashingTF = HashingTF(inputCol=\"filtered\", outputCol=\"rawFeatures\", numFeatures=10000)\n",
    "idf = IDF(inputCol=\"rawFeatures\", outputCol=\"features\", minDocFreq=5)\n",
    "w2v = Word2Vec(inputCol= 'filtered', outputCol= 'features', vectorSize= 100)\n",
    "label_stringIdx = StringIndexer(inputCol = \"polarity\", outputCol = \"label\")\n",
    "lr = LogisticRegression(maxIter=20, regParam=0.3, elasticNetParam=0)\n",
    "rf = RandomForestClassifier(labelCol=\"label\", featuresCol=\"features\", numTrees=10)\n",
    "nb = NaiveBayes(smoothing=1.0, modelType=\"multinomial\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "&nbsp;\n",
    "\n",
    "## Classification\n",
    "\n",
    "&nbsp;\n",
    "\n",
    "<p style=\"text-align:justify;\">Dans un premier temps, nous allons utiliser comme vectorisation du texte, la méthode CountVectorizer. Nous regarderons alors les résultats pour les différents algorithmes de classification.</p>\n",
    "\n",
    "&nbsp;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Nom du modèle utilisé : logistic_regression\n",
      "Score sur les données d'entraînement : 0.802\n",
      "Score sur les données test : 0.769 \n",
      "\n",
      "\n",
      "Nom du modèle utilisé : random_forest\n",
      "Score sur les données d'entraînement : 0.528\n",
      "Score sur les données test : 0.523 \n",
      "\n",
      "\n",
      "Nom du modèle utilisé : naive_bayes\n",
      "Score sur les données d'entraînement : 0.790\n",
      "Score sur les données test : 0.768 \n",
      "\n"
     ]
    }
   ],
   "source": [
    "predictions = {}\n",
    "models = [lr, rf, nb]\n",
    "name_models = ['logistic_regression', 'random_forest', 'naive_bayes']\n",
    "evaluator = MulticlassClassificationEvaluator(predictionCol=\"prediction\")\n",
    "for i, model in enumerate(models):\n",
    "    pipeline = Pipeline(stages=[regexTokenizer, stopwordsRemover, countVectors, label_stringIdx, model])\n",
    "    mod = pipeline.fit(df_train)\n",
    "    prediction_train = mod.transform(df_train)\n",
    "    prediction_test = mod.transform(df_test)\n",
    "    predictions['prediction_train_%s_%s'%(name_models[i],'CountVectorizer')] = prediction_train\n",
    "    predictions['prediction_test_%s_%s'%(name_models[i],'CountVectorizer')] = prediction_test\n",
    "    print(\"\\nNom du modèle utilisé : %s\" %(name_models[i]))\n",
    "    print(\"Score sur les données d'entraînement : %.3f\" %(evaluator.evaluate(prediction_train)))\n",
    "    print(\"Score sur les données test : %.3f \\n\" %(evaluator.evaluate(prediction_test)))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "&nbsp;\n",
    "\n",
    "<p style=\"text-align:justify;\">Faisons la même chose avec comme embedding TF-IDF.</p>\n",
    "\n",
    "&nbsp;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Nom du modèle utilisé : logistic_regression\n",
      "Score sur les données d'entraînement : 0.785\n",
      "Score sur les données test : 0.751 \n",
      "\n",
      "\n",
      "Nom du modèle utilisé : random_forest\n",
      "Score sur les données d'entraînement : 0.556\n",
      "Score sur les données test : 0.551 \n",
      "\n",
      "\n",
      "Nom du modèle utilisé : naive_bayes\n",
      "Score sur les données d'entraînement : 0.772\n",
      "Score sur les données test : 0.736 \n",
      "\n"
     ]
    }
   ],
   "source": [
    "models = [lr, rf, nb]\n",
    "name_models = ['logistic_regression', 'random_forest', 'naive_bayes']\n",
    "evaluator = MulticlassClassificationEvaluator(predictionCol=\"prediction\")\n",
    "for i, model in enumerate(models):\n",
    "    pipeline = Pipeline(stages=[regexTokenizer, stopwordsRemover, hashingTF, idf, label_stringIdx, model])\n",
    "    mod = pipeline.fit(df_train)\n",
    "    prediction_train = mod.transform(df_train)\n",
    "    prediction_test = mod.transform(df_test)\n",
    "    predictions['prediction_train_%s_%s'%(name_models[i],'tf_idf')] = prediction_train\n",
    "    predictions['prediction_test_%s_%s'%(name_models[i],'tf_idf')] = prediction_test\n",
    "    print(\"\\nNom du modèle utilisé : %s\" %(name_models[i]))\n",
    "    print(\"Score sur les données d'entraînement : %.3f\" %(evaluator.evaluate(prediction_train)))\n",
    "    print(\"Score sur les données test : %.3f \\n\" %(evaluator.evaluate(prediction_test)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "&nbsp;\n",
    "\n",
    "<p style=\"text-align:justify;\">Et enfin avec Word2vec. Nous ne pouvons pas utiliser naive bayes car les words embedding obtenus avec word2vec peuvent avoir des valeurs négatives.</p>\n",
    "\n",
    "&nbsp;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Nom du modèle utilisé : logistic_regression\n",
      "Score sur les données d'entraînement : 0.687\n",
      "Score sur les données test : 0.684 \n",
      "\n",
      "\n",
      "Nom du modèle utilisé : random_forest\n",
      "Score sur les données d'entraînement : 0.663\n",
      "Score sur les données test : 0.655 \n",
      "\n"
     ]
    }
   ],
   "source": [
    "models = [lr, rf]\n",
    "name_models = ['logistic_regression', 'random_forest']\n",
    "evaluator = MulticlassClassificationEvaluator(predictionCol=\"prediction\")\n",
    "for i, model in enumerate(models):\n",
    "    pipeline = Pipeline(stages=[regexTokenizer, stopwordsRemover, w2v, label_stringIdx, model])\n",
    "    mod = pipeline.fit(df_train)\n",
    "    prediction_train = mod.transform(df_train)\n",
    "    prediction_test = mod.transform(df_test)\n",
    "    predictions['prediction_train_%s_%s'%(name_models[i],'w2v')] = prediction_train\n",
    "    predictions['prediction_test_%s_%s'%(name_models[i],'w2v')] = prediction_test\n",
    "    print(\"\\nNom du modèle utilisé : %s\" %(name_models[i]))\n",
    "    print(\"Score sur les données d'entraînement : %.3f\" %(evaluator.evaluate(prediction_train)))\n",
    "    print(\"Score sur les données test : %.3f \\n\" %(evaluator.evaluate(prediction_test)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "&nbsp;\n",
    "\n",
    "<p style=\"text-align:justify;\">Les meilleurs scores possibles sont obtenus avec la vectorisation CountVectorizer pour les méthodes regression logistique et naive bayes (respectivement 0.769 et 0.768). On peut alors afficher les messages du jeu de données test avec le vrai label ainsi que la prédiction.</p>\n",
    "\n",
    "&nbsp;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------------------------+-----+----------+\n",
      "|                       message|label|prediction|\n",
      "+------------------------------+-----+----------+\n",
      "|! Et si affamé mais pas de ...|  1.0|       1.0|\n",
      "|! Identica présente actuell...|  1.0|       0.0|\n",
      "|! Je vais enfin m'endormir ...|  1.0|       1.0|\n",
      "|!?!? C'est un jour que j'ai...|  1.0|       1.0|\n",
      "|\"Easy\" qu Le plancher en bo...|  1.0|       0.0|\n",
      "|\"Empire du soleil\" L'auteur...|  1.0|       1.0|\n",
      "|\"Heart\" quot; N'est pas un ...|  1.0|       1.0|\n",
      "|\"I need\" & quot; -? V1-1333...|  1.0|       1.0|\n",
      "|\"Ils ont trouvé sonny? & Qu...|  1.0|       0.0|\n",
      "|\"Je vérifie mon twitter cha...|  1.0|       1.0|\n",
      "+------------------------------+-----+----------+\n",
      "only showing top 10 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "predictions['prediction_test_logistic_regression_CountVectorizer\"'] \\\n",
    "    .select(\"message\",\"label\",\"prediction\") \\\n",
    "    .show(n = 10, truncate = 30)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "&nbsp;\n",
    "\n",
    "## Cross validation\n",
    "\n",
    "&nbsp;\n",
    "\n",
    "<p style=\"text-align:justify;\">Afin d'améliorer encore le score en prédiction, nous pouvons utiliser la cross validation. Pour ce faire, nous utiliserons les fonctions ParamGridBuilder et CrossValidator de pyspark. Nous ferons une grid search seulement pour la regression logistique et la classification naïve bayésienne, pour un embedding via count vectorizer. </p>\n",
    "\n",
    "&nbsp;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from pyspark.ml.tuning import ParamGridBuilder, CrossValidator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Nom du modèle utilisé : logistic regression\n",
      "Score sur les données d'entraînement : 0.809\n",
      "Score sur les données test : 0.778 \n",
      "\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "lr = LogisticRegression()\n",
    "pipeline = Pipeline(stages=[regexTokenizer, stopwordsRemover, countVectors, label_stringIdx, lr])\n",
    "vectC = np.logspace(-3, -2, 20)\n",
    "paramGrid = (ParamGridBuilder()\n",
    "             .addGrid(lr.regParam, vectC) \n",
    "             .addGrid(lr.elasticNetParam, [0.1, 0.15]) \n",
    "             .build())\n",
    "\n",
    "evaluator = MulticlassClassificationEvaluator(predictionCol=\"prediction\")\n",
    "\n",
    "cv = CrossValidator(estimator=pipeline, \\\n",
    "                    estimatorParamMaps=paramGrid, \\\n",
    "                    evaluator=evaluator, \\\n",
    "                    numFolds=10)\n",
    "cvModel_lr = cv.fit(df_train)\n",
    "\n",
    "predictions_test = cvModel_lr.transform(df_test)\n",
    "predictions_train = cvModel_lr.transform(df_train)\n",
    "print(\"\\nNom du modèle utilisé : logistic regression\")\n",
    "print(\"Score sur les données d'entraînement : %.3f\" %(evaluator.evaluate(predictions_train)))\n",
    "print(\"Score sur les données test : %.3f \\n\" %(evaluator.evaluate(predictions_test)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Nom du modèle utilisé : naive bayes\n",
      "Score sur les données d'entraînement : 0.788\n",
      "Score sur les données test : 0.769\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "nb = NaiveBayes(modelType=\"multinomial\")\n",
    "pipeline = Pipeline(stages=[regexTokenizer, stopwordsRemover, countVectors, label_stringIdx, nb])\n",
    "paramGrid = (ParamGridBuilder()\n",
    "             .addGrid(nb.smoothing, [0,1,2,5,10]) \n",
    "             .build())\n",
    "\n",
    "evaluator = MulticlassClassificationEvaluator(predictionCol=\"prediction\")\n",
    "\n",
    "cv = CrossValidator(estimator=pipeline, \\\n",
    "                    estimatorParamMaps=paramGrid, \\\n",
    "                    evaluator=evaluator, \\\n",
    "                    numFolds=10)\n",
    "cvModel_nb = cv.fit(df_train)\n",
    "\n",
    "predictions_test = cvModel_nb.transform(df_test)\n",
    "predictions_train = cvModel_nb.transform(df_train)\n",
    "print(\"\\nNom du modèle utilisé : naive bayes\")\n",
    "print(\"Score sur les données d'entraînement : %.3f\" %(evaluator.evaluate(predictions_train)))\n",
    "print(\"Score sur les données test : %.3f\" %(evaluator.evaluate(predictions_test)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "&nbsp;\n",
    "\n",
    "<p style=\"text-align:justify;\">On a alors pu améliorer le score sur les données test en passant de 0.769 à 0.778 avec la regression logistiques. On peut alors afficher les paramètres optimaux. </p>\n",
    "\n",
    "&nbsp;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best regParam : 0.009\n",
      "\n",
      "Best elastic_net: 0.100\n"
     ]
    }
   ],
   "source": [
    "bestModel = cvModel_lr.bestModel\n",
    "param_dict = bestModel.stages[-1].extractParamMap()\n",
    "\n",
    "sane_dict = {}\n",
    "for k, v in param_dict.items():\n",
    "    sane_dict[k.name] = v\n",
    "\n",
    "print('Best regParam : %.3f' %sane_dict[\"regParam\"])\n",
    "print('\\nBest elastic_net: %.3f' %sane_dict[\"elasticNetParam\"])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "&nbsp;\n",
    "\n",
    "## Classification du fichier noclass.json\n",
    "\n",
    "&nbsp;"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "&nbsp;\n",
    "\n",
    "<p style=\"text-align:justify;\">On peut alors prédire le sentiment d'un nouveau message. On prédit donc le sentiment des messages du fichier noclass.json. </p>\n",
    "\n",
    "&nbsp;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+----------+\n",
      "|             message|prediction|\n",
      "+--------------------+----------+\n",
      "|! Et si affamé ma...|       1.0|\n",
      "|! Identica présen...|       0.0|\n",
      "|! Je vais enfin m...|       1.0|\n",
      "|!?!? C'est un jou...|       1.0|\n",
      "|\"Easy\" qu Le plan...|       0.0|\n",
      "|\"Empire du soleil...|       1.0|\n",
      "|\"Heart\" quot; N'e...|       1.0|\n",
      "|\"I need\" & quot; ...|       1.0|\n",
      "|\"Ils ont trouvé s...|       0.0|\n",
      "|\"Je vérifie mon t...|       0.0|\n",
      "|\"Recevoir un préa...|       0.0|\n",
      "|\"Statut sélectif ...|       1.0|\n",
      "|\"Sur la musique p...|       1.0|\n",
      "|\"The dating exper...|       0.0|\n",
      "|& Amp; Tout le re...|       1.0|\n",
      "|& Gt; Le frère es...|       1.0|\n",
      "|& Gt; Que cela im...|       0.0|\n",
      "|& Gt; __ & lt; Je...|       0.0|\n",
      "|& Lt; & lt; -----...|       1.0|\n",
      "|& Lt; --- ne peut...|       0.0|\n",
      "+--------------------+----------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_noclass = spark.read.json(\"noclass.json\")\n",
    "predictions_noclass = cvModel_lr.transform(df_test).select(\"message\",\"prediction\")\n",
    "predictions_noclass = predictions_noclass.withColumn(\"prediction\", predictions_noclass[\"prediction\"].cast('string'))\n",
    "predictions_noclass.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "&nbsp;\n",
    "\n",
    "<p style=\"text-align:justify;\">Puis on sauvegarde dans un fichier les résultats. </p>\n",
    "\n",
    "&nbsp;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions_noclass.coalesce(1).write.format('json').save('noclass_pred.json')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "&nbsp;\n",
    "\n",
    "## Distribution des calculs\n",
    "\n",
    "&nbsp;\n",
    "\n",
    "<p style=\"text-align:justify;\">Afin de suivre la progression des calculs de l'application, on peut utiliser une interface graphique \"Spark Web UI\" disponible à l'adresse http://localhost:4041. On peut alors voir tous les jobs exécutés et en cours d'exécution. En cliquant sur les jobs on a des indications sur la manière doit les jobs sont exécutés et on peut voir le temps d'exécution, le nombre de tasks par job etc. Voici un aperçu. </p>\n",
    "\n",
    "&nbsp;\n",
    "\n",
    "<p style=\"text-align:justify;\">Pendant l'exécution de la grid search pour la regression logistique, voici quelques jobs affichés :</p>\n",
    "\n",
    "&nbsp;\n",
    "<img src=\"https://drive.google.com/uc?id=1d_cOfMyK2pOzxXYWKc_sSfAZaafxqp4y\">\n",
    "\n",
    "&nbsp;\n",
    "\n",
    "<p style=\"text-align:justify;\">On peut alors retrouver les différentes étapes de notre pipeline avec Count Vectorizer, logistic regression, label_stringIdx etc. On peut alors voir que les calculs sont bien distribués pour toutes les étapes de la pipeline, soit dès qu'un dataframe est modifié. Affichons les étapes qui demandent le plus de temps.</p>\n",
    "\n",
    "&nbsp;\n",
    "<img src=\"https://drive.google.com/uc?id=1viF1j2BaA2sgZx3mHInaSsDHH5UPGK72\">\n",
    "\n",
    "&nbsp;\n",
    "\n",
    "<p style=\"text-align:justify;\">En plus de la regression logistique, il y a l'étape CountVectorizer qui demande pas mal de temps (1s). On pouvait s'y attendre car ces étapes demandent clairement plus de temps de calcul que les autres. On peut afficher le détail d'un job, prenons celui de la regression logistique.</p>\n",
    "\n",
    "&nbsp;\n",
    "\n",
    "<img src=\"https://drive.google.com/uc?id=1ZOnLu1jyzZq4UyYaXeXHCZvok-4jlzGQ\">\n",
    "\n",
    "&nbsp;\n",
    "\n",
    "<p style=\"text-align:justify;\">On peut voir que c'est une tâche ou la très grande partie du temps est consacrée uniquement aux calculs.</p>\n",
    "\n",
    "<p style=\"text-align:justify;\">On peut aussi afficher un graphique montrant les différents opérations (map, filter etc) sur les RDD lors de l'exécution d'une tâche.</p>\n",
    "&nbsp;\n",
    "\n",
    "<img src=\"https://drive.google.com/uc?id=1p7Pt2EwAYUJ3NrxUP7VlgH6oSjSAnLLZ\">\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
